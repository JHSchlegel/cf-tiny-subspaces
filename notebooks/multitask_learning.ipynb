{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Training Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive implementation of multi-task learning, applied to training models such as `MLP` and `CNN` on datasets like `Permuted MNIST` and `Split CIFAR-10/100`.\n",
    "\n",
    "While the primary focus of this study is on Continual Learning, this section takes a different approach by leveraging the entire dataset, processed in sequential chunks rather than task-by-task progression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Presets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from modules.MTLTrainer import MTLTrainer\n",
    "from modules.mlp import MLP\n",
    "from modules.cnn import CNN\n",
    "from modules.subspace_sgd import SubspaceSGD\n",
    "from utils.data_utils.permuted_mnist import PermutedMNIST\n",
    "from utils.data_utils.sequential_CIFAR import CL_CIFAR10, CL_CIFAR100\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function for Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/permuted_mnist.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_pmnist = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_pmnist.data.data_root = config_pmnist.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_pmnist.hydra.run.dir = config_pmnist.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config: DictConfig) -> None:\n",
    "    \"\"\"\n",
    "    Main training function for Permuted MNIST multi-task learning.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set the save directory\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create permuted MNIST datasets but combine them into one loader\n",
    "    pmnist = PermutedMNIST(num_tasks=config.data.num_tasks, seed=config.data.seed)\n",
    "    pmnist.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Combine all task datasets into one\n",
    "    combined_train_dataset = ConcatDataset([\n",
    "        pmnist.train_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "    combined_test_dataset = ConcatDataset([\n",
    "        pmnist.test_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "\n",
    "    # Create unified dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        combined_train_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        combined_test_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLP(\n",
    "        input_dim=config.model.input_dim,\n",
    "        output_dim=config.model.output_dim, \n",
    "        hidden_dim=config.model.hidden_dim,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SubspaceSGD(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        **OmegaConf.to_container(config.optimizer, resolve=True),\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train and evaluate\n",
    "        train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "        )\n",
    "\n",
    "        print(\"Training completed successfully!\")\n",
    "        print(f\"Final training accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "        print(f\"Final validation accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_pmnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function for Split CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/split_cifar10.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_scifar10 = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_scifar10.data.data_root = config_scifar10.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_scifar10.hydra.run.dir = config_scifar10.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config):\n",
    "    \"\"\"\n",
    "    Main training function for Split CIFAR multitask learning.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "    \"\"\"\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create sequential CIFAR-10 dataset\n",
    "    split_cifar10 = CL_CIFAR10(\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        seed=config.data.seed,\n",
    "    )\n",
    "    split_cifar10.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNN(\n",
    "        width=config.model.width,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SubspaceSGD(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        **OmegaConf.to_container(config.optimizer, resolve=True),\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        avg_train_accuracy, avg_val_accuracy = [], []\n",
    "        # Train and evaluate each task sequentially\n",
    "        for task_id in range(config.data.num_tasks):\n",
    "            print(f\"\\n=== Training Task {task_id + 1}/{config.data.num_tasks} ===\")\n",
    "            \n",
    "            # Get task-specific dataloaders\n",
    "            train_loader = split_cifar10.train_loaders[task_id]\n",
    "            val_loader = split_cifar10.test_loaders[task_id]\n",
    "\n",
    "            # Train and evaluate the task\n",
    "            train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "            )\n",
    "\n",
    "            avg_train_accuracy.append(train_metrics['accuracies'][-1])\n",
    "            avg_val_accuracy.append(val_metrics[\"accuracies\"][-1])\n",
    "\n",
    "            print(f\"Task {task_id + 1} Training Accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "            print(f\"Task {task_id + 1} Validation Accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "        # Calculate and print average accuracies\n",
    "        if avg_train_accuracy and avg_val_accuracy:\n",
    "            print(f\"\\n=== Average Multi-task Training Accuracy: {np.mean(avg_train_accuracy):.2f}% ===\")\n",
    "            print(f\"=== Average Multi-task Validation Accuracy: {np.mean(avg_val_accuracy):.2f}% ===\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    print(\"All tasks trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Split CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_scifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training function for Split CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/split_cifar100.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_scifar100 = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_scifar100.data.data_root = config_scifar100.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_scifar100.hydra.run.dir = config_scifar100.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config):\n",
    "    \"\"\"\n",
    "    Main training function for Split CIFAR multitask learning.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "    \"\"\"\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create sequential CIFAR-10 dataset\n",
    "    split_cifar10 = CL_CIFAR100(\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        seed=config.data.seed,\n",
    "    )\n",
    "    split_cifar10.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNN(\n",
    "        width=config.model.width,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SubspaceSGD(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        **OmegaConf.to_container(config.optimizer, resolve=True),\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        avg_train_accuracy, avg_val_accuracy = [], []\n",
    "        # Train and evaluate each task sequentially\n",
    "        for task_id in range(config.data.num_tasks):\n",
    "            print(f\"\\n=== Training Task {task_id + 1}/{config.data.num_tasks} ===\")\n",
    "            \n",
    "            # Get task-specific dataloaders\n",
    "            train_loader = split_cifar10.train_loaders[task_id]\n",
    "            val_loader = split_cifar10.test_loaders[task_id]\n",
    "\n",
    "            # Train and evaluate the task\n",
    "            train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "            )\n",
    "\n",
    "            avg_train_accuracy.append(train_metrics['accuracies'][-1])\n",
    "            avg_val_accuracy.append(val_metrics[\"accuracies\"][-1])\n",
    "\n",
    "            print(f\"Task {task_id + 1} Training Accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "            print(f\"Task {task_id + 1} Validation Accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "        # Calculate and print average accuracies\n",
    "        if avg_train_accuracy and avg_val_accuracy:\n",
    "            print(f\"\\n=== Average Multi-task Training Accuracy: {np.mean(avg_train_accuracy):.2f}% ===\")\n",
    "            print(f\"=== Average Multi-task Validation Accuracy: {np.mean(avg_val_accuracy):.2f}% ===\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    print(\"All tasks trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Split CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_scifar100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
