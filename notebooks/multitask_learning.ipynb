{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Training Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive implementation of multi-task learning, applied to training models such as `MLP` and `CNN` on datasets like `Permuted MNIST` and `Split CIFAR-10/100`.\n",
    "\n",
    "While the primary focus of this study is on Continual Learning, this section takes a different approach by leveraging the entire dataset, processed at once altogether/in sequential chunks rather than task-by-task progression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Presets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and logging setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "import warnings\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from modules.MTLTrainer import MTLTrainer\n",
    "from modules.mlp import MLP\n",
    "from modules.cnn import MultCNN\n",
    "from utils.data_utils.permuted_mnist import PermutedMNIST\n",
    "from utils.data_utils.sequential_CIFAR import CL_CIFAR10, CL_CIFAR100\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up logging:\n",
    "log_dir = \"../logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_filename = os.path.join(\n",
    "    log_dir, f'multitask_training_{datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}.log'\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function for Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/permuted_mnist.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_pmnist = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_pmnist.data.data_root = config_pmnist.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_pmnist.hydra.run.dir = config_pmnist.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config: DictConfig) -> None:\n",
    "    \"\"\"\n",
    "    Main training function for Permuted MNIST multi-task learning.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set the save directory\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    logging.info(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create permuted MNIST datasets but combine them into one loader\n",
    "    pmnist = PermutedMNIST(num_tasks=config.data.num_tasks, seed=config.data.seed)\n",
    "    pmnist.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Combine all task datasets into one\n",
    "    combined_train_dataset = ConcatDataset([\n",
    "        pmnist.train_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "    combined_test_dataset = ConcatDataset([\n",
    "        pmnist.test_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "\n",
    "    # Create unified dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        combined_train_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        combined_test_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLP(\n",
    "        input_dim=config.model.input_dim,\n",
    "        output_dim=config.model.output_dim, \n",
    "        hidden_dim=config.model.hidden_dim,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.optimizer.lr,\n",
    "        momentum=config.optimizer.momentum,\n",
    "        weight_decay=config.optimizer.weight_decay,\n",
    "        nesterov=config.optimizer.nesterov,\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train and evaluate\n",
    "        train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Training completed successfully!\")\n",
    "        logging.info(f\"Final Multi-task training accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "        logging.info(f\"Final Multi-task validation accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to c:/Users/rufat/cf-tiny-subspaces/notebooks/../results/permuted_mnist/subspace-None/k-10/batch_size-128/hidden_dim-100/lr-0.01/seed-42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrasadlii\u001b[0m (\u001b[33mml-projects\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\wandb\\run-20250105_174451-r2nke71b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml-projects/permuted_mnist_None/runs/r2nke71b' target=\"_blank\">dauntless-moon-2</a></strong> to <a href='https://wandb.ai/ml-projects/permuted_mnist_None' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml-projects/permuted_mnist_None' target=\"_blank\">https://wandb.ai/ml-projects/permuted_mnist_None</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml-projects/permuted_mnist_None/runs/r2nke71b' target=\"_blank\">https://wandb.ai/ml-projects/permuted_mnist_None/runs/r2nke71b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 4688/4688 [00:39<00:00, 119.48it/s, loss=1.4277, acc=55.50%]\n",
      "Evaluating: 100%|██████████| 782/782 [00:07<00:00, 105.77it/s, loss=0.5957, acc=81.95%]\n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\permuted_mnist\\subspace-None\\k-10\\batch_size-128\\hidden_dim-100\\lr-0.01\\seed-42\\models\\model_0.pt\n",
      "Epoch 1: 100%|██████████| 4688/4688 [00:32<00:00, 144.43it/s, loss=0.5190, acc=84.38%]\n",
      "Evaluating: 100%|██████████| 782/782 [00:07<00:00, 104.77it/s, loss=0.4398, acc=87.01%]\n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\permuted_mnist\\subspace-None\\k-10\\batch_size-128\\hidden_dim-100\\lr-0.01\\seed-42\\models\\model_1.pt\n",
      "Epoch 2: 100%|██████████| 4688/4688 [00:32<00:00, 145.65it/s, loss=0.4247, acc=87.37%]\n",
      "Evaluating: 100%|██████████| 782/782 [00:07<00:00, 101.50it/s, loss=0.3788, acc=88.72%]\n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\permuted_mnist\\subspace-None\\k-10\\batch_size-128\\hidden_dim-100\\lr-0.01\\seed-42\\models\\model_2.pt\n",
      "Epoch 3: 100%|██████████| 4688/4688 [00:32<00:00, 143.23it/s, loss=0.3713, acc=88.98%]\n",
      "Evaluating: 100%|██████████| 782/782 [00:07<00:00, 107.23it/s, loss=0.3329, acc=90.17%]\n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\permuted_mnist\\subspace-None\\k-10\\batch_size-128\\hidden_dim-100\\lr-0.01\\seed-42\\models\\model_3.pt\n",
      "Epoch 4: 100%|██████████| 4688/4688 [00:31<00:00, 150.16it/s, loss=0.3276, acc=90.30%]\n",
      "Evaluating: 100%|██████████| 782/782 [00:08<00:00, 95.61it/s, loss=0.2957, acc=91.22%] \n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\permuted_mnist\\subspace-None\\k-10\\batch_size-128\\hidden_dim-100\\lr-0.01\\seed-42\\models\\model_4.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n",
      "Final training accuracy: 90.30%\n",
      "Final validation accuracy: 91.22%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_pmnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function for Split CIFAR-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/split_cifar10.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_scifar10 = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_scifar10.data.data_root = config_scifar10.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_scifar10.hydra.run.dir = config_scifar10.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config):\n",
    "    \"\"\"\n",
    "    Main training function for Split CIFAR multitask learning.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "    \"\"\"\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    logging.info(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create sequential CIFAR-10 dataset\n",
    "    split_cifar10 = CL_CIFAR10(\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        seed=config.data.seed,\n",
    "    )\n",
    "    split_cifar10.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Combine all task datasets into one\n",
    "    combined_train_dataset = ConcatDataset([\n",
    "        split_cifar10.train_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "    combined_test_dataset = ConcatDataset([\n",
    "        split_cifar10.test_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "\n",
    "    # Create unified dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        combined_train_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        combined_test_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MultCNN(\n",
    "        width=config.model.width,\n",
    "        num_classes=config.data.num_tasks * config.data.classes_per_task,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.optimizer.lr,\n",
    "        momentum=config.optimizer.momentum,\n",
    "        weight_decay=config.optimizer.weight_decay,\n",
    "        nesterov=config.optimizer.nesterov,\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train and evaluate\n",
    "        train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Training completed successfully!\")\n",
    "        logging.info(f\"Final Multi-task training accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "        logging.info(f\"Final Multi-task validation accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Split CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to c:/Users/rufat/cf-tiny-subspaces/notebooks/../results/split_cifar10/subspace-None/k-10/batch_size-32/width-32/lr-0.001/seed-42\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrasadlii\u001b[0m (\u001b[33mml-projects\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\wandb\\run-20250105_190122-dwzfjz4u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml-projects/split_cifar10None/runs/dwzfjz4u' target=\"_blank\">crisp-feather-2</a></strong> to <a href='https://wandb.ai/ml-projects/split_cifar10None' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml-projects/split_cifar10None' target=\"_blank\">https://wandb.ai/ml-projects/split_cifar10None</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml-projects/split_cifar10None/runs/dwzfjz4u' target=\"_blank\">https://wandb.ai/ml-projects/split_cifar10None/runs/dwzfjz4u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1563/1563 [00:55<00:00, 28.41it/s, loss=0.7087, acc=58.02%]\n",
      "Evaluating: 100%|██████████| 313/313 [00:19<00:00, 15.88it/s, loss=0.6829, acc=57.25%]\n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\split_cifar10\\subspace-None\\k-10\\batch_size-32\\width-32\\lr-0.001\\seed-42\\models\\model_0.pt\n",
      "Epoch 1: 100%|██████████| 1563/1563 [00:50<00:00, 31.17it/s, loss=0.6534, acc=61.82%] \n",
      "Evaluating: 100%|██████████| 313/313 [00:19<00:00, 15.91it/s, loss=0.6477, acc=63.26%]\n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\split_cifar10\\subspace-None\\k-10\\batch_size-32\\width-32\\lr-0.001\\seed-42\\models\\model_1.pt\n",
      "Epoch 2: 100%|██████████| 1563/1563 [00:48<00:00, 32.32it/s, loss=0.6401, acc=63.25%] \n",
      "Evaluating: 100%|██████████| 313/313 [01:30<00:00,  3.46it/s, loss=0.6411, acc=63.67%] \n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\split_cifar10\\subspace-None\\k-10\\batch_size-32\\width-32\\lr-0.001\\seed-42\\models\\model_2.pt\n",
      "Epoch 3: 100%|██████████| 1563/1563 [00:46<00:00, 33.66it/s, loss=0.6329, acc=64.01%]\n",
      "Evaluating: 100%|██████████| 313/313 [00:21<00:00, 14.69it/s, loss=0.6232, acc=65.52%]\n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\split_cifar10\\subspace-None\\k-10\\batch_size-32\\width-32\\lr-0.001\\seed-42\\models\\model_3.pt\n",
      "Epoch 4: 100%|██████████| 1563/1563 [00:47<00:00, 32.98it/s, loss=0.6289, acc=64.39%]\n",
      "Evaluating: 100%|██████████| 313/313 [00:23<00:00, 13.30it/s, loss=0.6601, acc=62.10%] \n",
      "INFO:root:Checkpoint saved: c:\\Users\\rufat\\cf-tiny-subspaces\\notebooks\\..\\results\\split_cifar10\\subspace-None\\k-10\\batch_size-32\\width-32\\lr-0.001\\seed-42\\models\\model_4.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n",
      "Final training accuracy: 64.40%\n",
      "Final validation accuracy: 62.10%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_scifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training function for Split CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/split_cifar100.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_scifar100 = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_scifar100.data.data_root = config_scifar100.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_scifar100.hydra.run.dir = config_scifar100.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config):\n",
    "    \"\"\"\n",
    "    Main training function for Split CIFAR multitask learning.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "    \"\"\"\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    logging.info(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create sequential CIFAR-10 dataset\n",
    "    split_cifar10 = CL_CIFAR100(\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        seed=config.data.seed,\n",
    "    )\n",
    "    split_cifar10.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Combine all task datasets into one\n",
    "    combined_train_dataset = ConcatDataset([\n",
    "        split_cifar10.train_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "    combined_test_dataset = ConcatDataset([\n",
    "        split_cifar10.test_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "\n",
    "    # Create unified dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        combined_train_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        combined_test_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MultCNN(\n",
    "        width=config.model.width,\n",
    "        num_classes=config.data.num_tasks * config.data.classes_per_task,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.optimizer.lr,\n",
    "        momentum=config.optimizer.momentum,\n",
    "        weight_decay=config.optimizer.weight_decay,\n",
    "        nesterov=config.optimizer.nesterov,\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train and evaluate\n",
    "        train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Training completed successfully!\")\n",
    "        logging.info(f\"Final Multi-task training accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "        logging.info(f\"Final Multi-task validation accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Split CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_scifar100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
