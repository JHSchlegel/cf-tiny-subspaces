{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Training Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive implementation of multi-task learning, applied to training models such as `MLP` and `CNN` on datasets like `Permuted MNIST` and `Split CIFAR-10/100`.\n",
    "\n",
    "While the primary focus of this study is on Continual Learning, this section takes a different approach by leveraging the entire dataset, processed at once altogether/in sequential chunks rather than task-by-task progression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Presets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "import warnings\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from modules.MTLTrainer import MTLTrainer\n",
    "from modules.mlp import MLP\n",
    "from modules.cnn import CNN\n",
    "from modules.subspace_sgd import SubspaceSGD\n",
    "from utils.data_utils.permuted_mnist import PermutedMNIST\n",
    "from utils.data_utils.sequential_CIFAR import CL_CIFAR10, CL_CIFAR100\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function for Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/permuted_mnist.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_pmnist = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_pmnist.data.data_root = config_pmnist.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_pmnist.hydra.run.dir = config_pmnist.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config: DictConfig) -> None:\n",
    "    \"\"\"\n",
    "    Main training function for Permuted MNIST multi-task learning.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set the save directory\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create permuted MNIST datasets but combine them into one loader\n",
    "    pmnist = PermutedMNIST(num_tasks=config.data.num_tasks, seed=config.data.seed)\n",
    "    pmnist.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Combine all task datasets into one\n",
    "    combined_train_dataset = ConcatDataset([\n",
    "        pmnist.train_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "    combined_test_dataset = ConcatDataset([\n",
    "        pmnist.test_loaders[task_id].dataset for task_id in range(config.data.num_tasks)\n",
    "    ])\n",
    "\n",
    "    # Create unified dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        combined_train_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        combined_test_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.data.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLP(\n",
    "        input_dim=config.model.input_dim,\n",
    "        output_dim=config.model.output_dim, \n",
    "        hidden_dim=config.model.hidden_dim,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.optimizer.lr,\n",
    "        momentum=config.optimizer.momentum,\n",
    "        weight_decay=config.optimizer.weight_decay,\n",
    "        nesterov=config.optimizer.nesterov,\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train and evaluate\n",
    "        train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "        )\n",
    "\n",
    "        print(\"Training completed successfully!\")\n",
    "        print(f\"Final training accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "        print(f\"Final validation accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to c:/Users/rufat/cf-tiny-subspaces/notebooks/../results/permuted_mnist/subspace-None/k-10/batch_size-128/hidden_dim-100/lr-0.01/seed-42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/4688 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training failed: SGD.step() got an unexpected keyword argument 'fp16'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SGD.step() got an unexpected keyword argument 'fp16'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_pmnist\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 105\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     85\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MTLTrainer(\n\u001b[0;32m     86\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     87\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     wandb_config\u001b[38;5;241m=\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mto_container(config, resolve\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     train_metrics, val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal training accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracies\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rufat\\cf-tiny-subspaces\\modules\\MTLTrainer.py:269\u001b[0m, in \u001b[0;36mMTLTrainer.train_and_evaluate\u001b[1;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03mMain training and evaluation loop.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    Tuple containing training and validation metrics\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# Store training metrics\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\rufat\\cf-tiny-subspaces\\modules\\MTLTrainer.py:155\u001b[0m, in \u001b[0;36mMTLTrainer._train_epoch\u001b[1;34m(self, train_loader, epoch)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    153\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubspace_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubspace_type\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n\u001b[0;32m    161\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\rufat\\cf-tiny-subspaces\\cf\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rufat\\cf-tiny-subspaces\\cf\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "\u001b[1;31mTypeError\u001b[0m: SGD.step() got an unexpected keyword argument 'fp16'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_pmnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function for Split CIFAR-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/split_cifar10.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_scifar10 = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_scifar10.data.data_root = config_scifar10.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_scifar10.hydra.run.dir = config_scifar10.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config):\n",
    "    \"\"\"\n",
    "    Main training function for Split CIFAR multitask learning.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "    \"\"\"\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create sequential CIFAR-10 dataset\n",
    "    split_cifar10 = CL_CIFAR10(\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        seed=config.data.seed,\n",
    "    )\n",
    "    split_cifar10.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNN(\n",
    "        width=config.model.width,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.optimizer.lr,\n",
    "        momentum=config.optimizer.momentum,\n",
    "        weight_decay=config.optimizer.weight_decay,\n",
    "        nesterov=config.optimizer.nesterov,\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        avg_train_accuracy, avg_val_accuracy = [], []\n",
    "        # Train and evaluate each task sequentially\n",
    "        for task_id in range(config.data.num_tasks):\n",
    "            print(f\"\\n=== Training Task {task_id + 1}/{config.data.num_tasks} ===\")\n",
    "            \n",
    "            # Get task-specific dataloaders\n",
    "            train_loader = split_cifar10.train_loaders[task_id]\n",
    "            val_loader = split_cifar10.test_loaders[task_id]\n",
    "\n",
    "            # Train and evaluate the task\n",
    "            train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "            )\n",
    "\n",
    "            avg_train_accuracy.append(train_metrics['accuracies'][-1])\n",
    "            avg_val_accuracy.append(val_metrics[\"accuracies\"][-1])\n",
    "\n",
    "            print(f\"Task {task_id + 1} Training Accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "            print(f\"Task {task_id + 1} Validation Accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "        # Calculate and print average accuracies\n",
    "        if avg_train_accuracy and avg_val_accuracy:\n",
    "            print(f\"\\n=== Average Multi-task Training Accuracy: {np.mean(avg_train_accuracy):.2f}% ===\")\n",
    "            print(f\"=== Average Multi-task Validation Accuracy: {np.mean(avg_val_accuracy):.2f}% ===\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    print(\"All tasks trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Split CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_scifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training function for Split CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML config as a plain text file\n",
    "with open(\"../configs/split_cifar100.yaml\", \"r\") as f:\n",
    "    config_str = f.read()\n",
    "\n",
    "# Replace Hydra-style placeholders and convert backslashes to forward slashes (e.g. Windows-like, change for Linux/MacOS)\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "config_str = config_str.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "# Load the updated config into OmegaConf\n",
    "config_scifar100 = OmegaConf.create(config_str)\n",
    "\n",
    "# Dynamically resolve Hydra-style paths\n",
    "config_scifar100.data.data_root = config_scifar100.data.data_root.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "config_scifar100.hydra.run.dir = config_scifar100.hydra.run.dir.replace(\"${hydra:runtime.cwd}\", cwd)\n",
    "\n",
    "\n",
    "# Define the main training function\n",
    "def main(config):\n",
    "    \"\"\"\n",
    "    Main training function for Split CIFAR multitask learning.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object loaded from YAML.\n",
    "    \"\"\"\n",
    "    save_dir = config.hydra.run.dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\"Saving results to {save_dir}\")\n",
    "\n",
    "    # Create sequential CIFAR-10 dataset\n",
    "    split_cifar10 = CL_CIFAR100(\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        seed=config.data.seed,\n",
    "    )\n",
    "    split_cifar10.setup_tasks(\n",
    "        batch_size=config.data.batch_size,\n",
    "        data_root=config.data.data_root,\n",
    "        num_workers=config.data.num_workers,\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNN(\n",
    "        width=config.model.width,\n",
    "        num_tasks=config.data.num_tasks,\n",
    "        classes_per_task=config.data.classes_per_task,\n",
    "    )\n",
    "    model.to(config.training.device)\n",
    "\n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.optimizer.lr,\n",
    "        momentum=config.optimizer.momentum,\n",
    "        weight_decay=config.optimizer.weight_decay,\n",
    "        nesterov=config.optimizer.nesterov,\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = MTLTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        save_dir=save_dir,\n",
    "        num_epochs=config.training.num_epochs,\n",
    "        log_interval=config.training.log_interval,\n",
    "        eval_freq=config.training.eval_freq,\n",
    "        checkpoint_freq=config.training.checkpoint_freq,\n",
    "        seed=config.training.seed,\n",
    "        subspace_type=config.training.subspace_type,\n",
    "        scheduler=None,\n",
    "        device=config.training.device,\n",
    "        use_wandb=config.wandb.enabled,\n",
    "        wandb_project=config.wandb.project,\n",
    "        wandb_config=OmegaConf.to_container(config, resolve=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        avg_train_accuracy, avg_val_accuracy = [], []\n",
    "        # Train and evaluate each task sequentially\n",
    "        for task_id in range(config.data.num_tasks):\n",
    "            print(f\"\\n=== Training Task {task_id + 1}/{config.data.num_tasks} ===\")\n",
    "            \n",
    "            # Get task-specific dataloaders\n",
    "            train_loader = split_cifar10.train_loaders[task_id]\n",
    "            val_loader = split_cifar10.test_loaders[task_id]\n",
    "\n",
    "            # Train and evaluate the task\n",
    "            train_metrics, val_metrics = trainer.train_and_evaluate(\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "            )\n",
    "\n",
    "            avg_train_accuracy.append(train_metrics['accuracies'][-1])\n",
    "            avg_val_accuracy.append(val_metrics[\"accuracies\"][-1])\n",
    "\n",
    "            print(f\"Task {task_id + 1} Training Accuracy: {train_metrics['accuracies'][-1]:.2f}%\")\n",
    "            print(f\"Task {task_id + 1} Validation Accuracy: {val_metrics['accuracies'][-1]:.2f}%\")\n",
    "\n",
    "        # Calculate and print average accuracies\n",
    "        if avg_train_accuracy and avg_val_accuracy:\n",
    "            print(f\"\\n=== Average Multi-task Training Accuracy: {np.mean(avg_train_accuracy):.2f}% ===\")\n",
    "            print(f\"=== Average Multi-task Validation Accuracy: {np.mean(avg_val_accuracy):.2f}% ===\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    print(\"All tasks trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and evaluation for Split CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config=config_scifar100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
